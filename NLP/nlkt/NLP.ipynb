{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e1348-7731-44ba-af52-6af5bd787ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "+    combines Maths, Liguistics, and Computer Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed3eec-c409-4994-a314-e852cf198a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  why is NLP difficult\n",
    "+    due to nature of language itself, it is highly contextual, ambiguious, has methaphors and expressions\n",
    "\n",
    "### Syntactic Ambiguity (prepositional phrase attachment problem)\n",
    "+    News headline \"Stolen painting found by tree\" could mean\n",
    "> 1. By a tree, a stolen painting was found  \n",
    "> 2. Tree found a stolen painting  \n",
    "+    Interpretation depends on what the prepositional phrase (by tree) modifying  \n",
    "> If it modifies noun phrase \"stolen painting\", #1 above is correct  \n",
    "> If it modifies verb phrase \"found\", #2 above is correct  \n",
    "+    Example of preposition phrase attachment problem - form of syntactic ambiguity\n",
    "\n",
    "### Lexical ambiguity\n",
    "+    \"He's at the bank\" - is it financial bank or river bank\n",
    "+    \"She flew in last night\" - took a flight not flew on wings\n",
    "\n",
    "### Pragmatics (how people use language)\n",
    "+    Tones & guestures are equally important\n",
    "\n",
    "### multi-purpose tool\n",
    "+    to inform, comfort, entertain, persuade, manipulate...\n",
    "+    Also depending on culture, people say one thing and it might mean something else\n",
    "\n",
    "### Humans can master these ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577669b-78ea-46f5-9d75-c6388606574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP progress\n",
    "\n",
    "+    1950s - rules based system\n",
    "+    1980s - statistical revolution/ ML\n",
    "+    2010s - Neural networks/ deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ab801-a744-44b2-a40d-cfc1e4ce3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries available\n",
    "+    PyTorch\n",
    "+    NLKT\n",
    "+    SpaCy\n",
    "+    TensorFlow\n",
    "+    AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d576b-16b8-4ced-a50f-51a53ee2da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing - tokenization, simplification techniques, taggin, simple rules-based approaches\n",
    "Basic Vectorization - turning text into numbers, measuring similarity between documents\n",
    "Modelling overview - types of ML, algorithms vs models, evaluation...\n",
    "\n",
    "Neural Networks - \n",
    "Word Vectors - \n",
    "Recurrent Neural Networks - capture sequence information and generate language\n",
    "Seq2Seq and Attention - transform one sequence to another\n",
    "Transformer models - dominant mainstream architecture, pretraining and transfer learning\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf712-3418-4c6c-aac4-c8d4ca0cb12b",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ecc3b-fdb0-4c73-b8aa-d7ab01124660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "+    use spaces to tokenize\n",
    "+    Tokenize by sentence\n",
    "+    Tokenize by word (smallest unit of speech that carries a meaning on its own), punctuation, numbers\n",
    "+    Morpheme - smallest unit of speech but does not carry a meaning on its own (-ing, pre- un- re-)\n",
    "+    Grapheme - smallest functional unit in a writing system. In English, it is letters\n",
    "\n",
    "### Challenges\n",
    "+    Punctuations do not seperate\n",
    "+    words with apostrophe do not seperate\n",
    "+    does not work with apply to languages without whitespace\n",
    "+    cannot seperate acronyms\n",
    "+    what is a word? \"full moon\" = one word or two. \"didn't\" = should this be split into two\n",
    "+    \"It's\" will split at the apostrophe to give you 'It' and \"'s\",\n",
    "+    punctuation is split as separate word\n",
    "\n",
    "### Libraries: SpaCy, NLKT, AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dd355-576b-4d46-9096-156bc961f247",
   "metadata": {},
   "source": [
    "# Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb966f2-3dd1-475d-92dd-8e4ed7fc8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case-folding\n",
    "+    Could result in information loss - noun would equal to a verb (cook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812f7ae-5f65-4c31-8508-f5ad72613a6a",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506b945-bb88-459b-95e4-04e2c612cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop word removal\n",
    "+    words that occur frequently but carry little information (the, a, of, an, this, that...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac8fef-d7e5-4675-a7a5-98dc9afa1252",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54c588-c68e-4fd6-89fa-9148965cb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming\n",
    "+    remove word of suffixes and maybe prefixes\n",
    "+    might result in words that do not make sense (Analysis > Analysi)\n",
    "+    overstem (university and universe > univers).  when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive\n",
    "+    understam (alumnus and alumni > alumnu and alumni). when two related words should be reduced to the same stem but aren’t. This is a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed95d2c-6a54-4698-884e-dca3705392f4",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b7182-dfe8-47cf-b32b-82b9574313d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "+    reduce word to lemma\n",
    "+    A **lemma** is a word that represents a whole group of words, and that group of words is called a lexeme\n",
    "+    more sophisticated than stemming; stemming does not consider synonyms, tense, POS (talkative > talkative because it is a verb) (stemmer: talkative > talk)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5819f-d0be-4643-b272-6694b5d6fbe3",
   "metadata": {},
   "source": [
    "# Parts of Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f71f48-edb2-4cd1-90ac-c2da0c1303ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parts of Speech (POS) tagging\n",
    "+    classifying how a word is used in a sentence\n",
    "+    discover intent or action\n",
    "+    \"I want to book a hotel room\" (book = verb)\n",
    "+    \"I left a book in a hotel room\" (book = noun)\n",
    "+    help with word sense disambiguation determining which meaning of a word is intended based on its context or grammatical relationship with neighbouring words\n",
    "+    In English, there are eight parts of speech:\n",
    ">Part of speech | Role | Examples  \n",
    ">Noun | Is a person, place, or thing | mountain, bagel, Poland  \n",
    ">Pronoun | Replaces a noun | you, she, we\n",
    ">Adjective | Gives information about what a noun is like | efficient, windy, colorful  \n",
    ">Verb | Is an action or a state of being | learn, is, go  \n",
    ">Adverb | Gives information about a verb, an adjective, or another adverb | efficiently, always, very  \n",
    ">Preposition | Gives information about how a noun or pronoun is connected to another word | from, about, at  \n",
    ">Conjunction | Connects two other words or phrases | so, because, and  \n",
    ">Interjection | Is an exclamation | yay, ow, wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2171f28-ff28-49a7-b1d4-b0d85e599815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\rajes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dd637-ca23-4e53-9f5a-4ee67b85cb81",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e1879-efd1-48f6-be9a-820ea4ac14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named Entity Recognition\n",
    "+    anything that can be referred as a proper noun POS\n",
    "+    can span multiple tokens, type entity (Hamilton = name, place, company, musical???)\n",
    "+    kind of entity and span; BIO (beginning, inside, outside of entity)\n",
    "+    NER use Hidden Markov models (HMM); Conditional Random Field (CRF)\n",
    "+    NER now use neural network architectures Recurrent Neural Network (RNN), Transformer\n",
    "+    Named Entity Type\n",
    ">NE type | Examples  \n",
    ">ORGANIZATION | Georgia-Pacific Corp., WHO  \n",
    ">PERSON | Eddy Bonte, President Obama  \n",
    ">LOCATION | Murray River, Mount Everest  \n",
    ">DATE | June, 2008-06-29  \n",
    ">TIME | two fifty a m, 1:30 p.m.  \n",
    ">MONEY | 175 million Canadian dollars, GBP 10.40  \n",
    ">PERCENT | twenty pct, 18.75 %  \n",
    ">FACILITY | Washington Monument, Stonehenge  \n",
    ">GPE | South East Asia, Midlothian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9927c9-6409-4bc6-ae34-e9646914e383",
   "metadata": {},
   "source": [
    "# Chunking & Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b13dd-4606-4ae7-ab47-7ab83ce3f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "+    allows you to identify phrases\n",
    "+    chunking makes use of POS tags to group words and apply chunk tags to those groups\n",
    "+    chunk grammar is a combination of rules on how sentences should be chunked\n",
    "\n",
    "# Chinking\n",
    "+    Chinking is used together with chunking, but while chunking is used to include a pattern, chinking is used to exclude a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed86769-a59d-492d-945e-ba417bacec30",
   "metadata": {},
   "source": [
    "# Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54a35a-96af-4002-a370-426ca9545788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance\n",
    "+    When you use a concordance, you can see each time a word is used, along with its immediate context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29983c8c-52e3-44c6-a737-de182e3ff901",
   "metadata": {},
   "source": [
    "# Dispersion Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811aa14-e1c3-49db-84d1-c2a41d6e5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "+    see how much a particular word appears and where it appears. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf98b7-88d3-49c1-a4ee-8f2f597cd667",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44093be-33de-480d-96db-2a43df63d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "+    you can check which words show up most frequently in your text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca1a86-2a58-4864-bdf5-fea8ed22836a",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332246b-d06e-4adc-8766-be7cb85e2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocations\n",
    "+    A collocation is a sequence of words that shows up often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20dd82-e40b-43b0-8888-1e8b41db9ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625730c-9c8e-411a-b03c-2e0ba3e93a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348eb8d-ec9a-45b9-bf43-53448c4ea379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a799f7-27c6-40e5-aaa0-acb303b38b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0606d04-1c0c-494e-a3ae-1f5c3c828708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ce5f6-e480-42bb-8cdd-32794e23f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing\n",
    "+   Determine the syntactic structure of a sentence\n",
    "+    Constituency parsing creates a parse tree (sentence > phrases and clauses > words)\n",
    "+    Example\n",
    ">She (NP) enrolled in the course at the university (VP)  \n",
    ">She (PRP) enrolled (VBP) in the course (PP) at the university (PP)  \n",
    ">in (IN) the course (NP) at (IN) the university (NP)  \n",
    ">the (DT) course (NN) the (DT) university (NN)  \n",
    "+    Dependency parsing done using dependency grammer\n",
    ">useful in grammer checking, question answering, semantic parsing\n",
    "+    Parsing + POS + NER = information extraction system\n",
    "+    Constituency vs Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b43e1-a1f2-4fc8-a065-b20dfe8f106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorization\n",
    "+    translate pre-processed text into vectors\n",
    "+    Classic ML pipeline: pre-processing > Feature Selection & Extraction > Modelling > Prediction/ Inference\n",
    "+    Feature - any property that is useful for making predictions or explaining relationships\n",
    "+    Feature selection = woud count, document ago, author ID or some metadata\n",
    "+    Feature extraction = combining or reducing features through dimensionality reduction\n",
    "+    We want to end up with a matrix - each row is a document (instance or feature vector), each column is a feature\n",
    "+    for e.g. 100 documents + 5 features = matrix 100 x 5\n",
    "+    feature selection and extraction knowledge = feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a82b72-59ef-4af4-87f4-a64955fad270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag-of-words\n",
    "+    describing document by word occurrences\n",
    "+    if two documents have a lot of overlapping vocabulary, then they likely belong to the same class\n",
    "+    term document matrix (Binary BOW) - row is a document, column is a word, 1 = word occurs in a set of documents; 0 = does not\n",
    "+    Each feature vendor is a point in a miltidimensional space (Vector Space Model)\n",
    "+    If 2 documents have similar vocabulary, they cluster in VSM (useful for relevance ranking, plagiarism detection, document classification etc.)\n",
    "+    Dot Product (basis for vector similarity matrix) + normalise vector length (L2 norm) + cosine similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4ce4d-cb9e-46e6-b120-c173c6daaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n-grams\n",
    "+    chunk of continuous tokens\n",
    "+    2-gram has 2 tokens per chunk etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
