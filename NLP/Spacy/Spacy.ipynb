{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5afe670d-4d8a-462e-b1a9-09792fa87835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP is a subfield of artificial intelligence, and it’s all about allowing computers to comprehend human language. NLP involves analyzing, quantifying, understanding, and deriving meaning from natural languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e737ef-c7af-4c20-933a-f3b63ad783ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a book\n",
    "enc='utf-8'\n",
    "with open(\"Lincoln_Second_Inaugural.txt\",\"r\",encoding=enc) as f:\n",
    "    book=f.read()\n",
    "    #print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304ec1b9-c825-43e4-96f4-99aee044d53a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Import spacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## Pipelines are trained on large datasets of labeled example texts.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## Create (nlp) pipeline (tok2vec, tagger, parser, attribute_ruler, lemmatizer, ner)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## by loading small / medium/ large/ transformer pipeline package\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m## The package provides the binary weights that enable spaCy to make predictions. \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m## includes the vocabulary, meta information about the pipeline and the configuration file used to train it.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\config.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\types.py:25\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Any,\n\u001b[0;32m      6\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     overload,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[0;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\compat.py:99\u001b[0m\n\u001b[0;32m     95\u001b[0m has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     h5py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:45\u001b[0m\n\u001b[0;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[0;32m     40\u001b[0m     ))\n\u001b[0;32m     43\u001b[0m _errors\u001b[38;5;241m.\u001b[39msilence_errors()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_converters \u001b[38;5;28;01mas\u001b[39;00m _register_converters, \\\n\u001b[0;32m     46\u001b[0m                    unregister_converters \u001b[38;5;28;01mas\u001b[39;00m _unregister_converters\n\u001b[0;32m     47\u001b[0m _register_converters()\n\u001b[0;32m     48\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_unregister_converters)\n",
      "File \u001b[1;32mh5py\\\\_conv.pyx:1\u001b[0m, in \u001b[0;36minit h5py._conv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5r.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5r\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5p.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5p\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "## Import spacy\n",
    "import spacy\n",
    "\n",
    "## Pipelines are trained on large datasets of labeled example texts.\n",
    "## Create (nlp) pipeline (tok2vec, tagger, parser, attribute_ruler, lemmatizer, ner)\n",
    "## by loading small / medium/ large/ transformer pipeline package\n",
    "## The package provides the binary weights that enable spaCy to make predictions. \n",
    "## includes the vocabulary, meta information about the pipeline and the configuration file used to train it.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "## returns a language object containing all components and data needed to process text\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40ec244a-bc05-498c-84fe-4a09fde7662c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x236f41ed9d0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x236f41eec30>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x236ed34cd60>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x236ed61df90>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x236ed5b0790>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x236ed34cf90>)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74f333fb-df13-4670-b519-30baf89de25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow countrymen: at this second appearing to take the oath of the presidential office there is less occasion for an extended address than there was at the first.\n",
      "Then a statement somewhat in detail of a course to be pursued seemed fitting and proper.\n",
      "\n",
      "\n",
      "With malice toward none with charity for all with firmness in the right as God gives us to see the right let us strive on to finish the work we are in to bind up the nation's wounds, to care for him who shall have borne the battle and for his widow and his orphan ~ to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.\n"
     ]
    }
   ],
   "source": [
    "## process text with nlp object to create a doc object\n",
    "doc=nlp(book)\n",
    "\n",
    "## sentence detection\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5543f03c-4306-42a5-bd53-251d51b63082",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'book' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(book)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mtype\u001b[39m(doc)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'book' is not defined"
     ]
    }
   ],
   "source": [
    "## tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects.\n",
    "doc=nlp(book)\n",
    "type(doc)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.idx)                       #.idx is starting posiiton in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "696d5bba-ed40-427b-bdfc-9a16caf583cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fellow countrymen: at this second appearing"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Span object is a slice of the document consisting of one or more tokens. \n",
    "span=doc[0:7]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "292f0895-0944-4260-8f39-81efd7488dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                     Is Alphanumeric?          Is Punctuation?           Like Number?             \n",
      "Fellow                    0                         True                      False                     False                    \n",
      "countrymen                1                         True                      False                     False                    \n",
      ":                         2                         False                     True                      False                    \n",
      "at                        3                         True                      False                     False                    \n",
      "this                      4                         True                      False                     False                    \n",
      "second                    5                         True                      False                     True                     \n",
      "appearing                 6                         True                      False                     False                    \n",
      "to                        7                         True                      False                     False                    \n",
      "take                      8                         True                      False                     False                    \n",
      "the                       9                         True                      False                     False                    \n",
      "oath                      10                        True                      False                     False                    \n",
      "of                        11                        True                      False                     False                    \n",
      "the                       12                        True                      False                     False                    \n",
      "presidential              13                        True                      False                     False                    \n",
      "office                    14                        True                      False                     False                    \n",
      "there                     15                        True                      False                     False                    \n",
      "is                        16                        True                      False                     False                    \n",
      "less                      17                        True                      False                     False                    \n",
      "occasion                  18                        True                      False                     False                    \n",
      "for                       19                        True                      False                     False                    \n",
      "an                        20                        True                      False                     False                    \n",
      "extended                  21                        True                      False                     False                    \n",
      "address                   22                        True                      False                     False                    \n",
      "than                      23                        True                      False                     False                    \n",
      "there                     24                        True                      False                     False                    \n",
      "was                       25                        True                      False                     False                    \n",
      "at                        26                        True                      False                     False                    \n",
      "the                       27                        True                      False                     False                    \n",
      "first                     28                        True                      False                     True                     \n",
      ".                         29                        False                     True                      False                    \n",
      "Then                      30                        True                      False                     False                    \n",
      "a                         31                        True                      False                     False                    \n",
      "statement                 32                        True                      False                     False                    \n",
      "somewhat                  33                        True                      False                     False                    \n",
      "in                        34                        True                      False                     False                    \n",
      "detail                    35                        True                      False                     False                    \n",
      "of                        36                        True                      False                     False                    \n",
      "a                         37                        True                      False                     False                    \n",
      "course                    38                        True                      False                     False                    \n",
      "to                        39                        True                      False                     False                    \n",
      "be                        40                        True                      False                     False                    \n",
      "pursued                   41                        True                      False                     False                    \n",
      "seemed                    42                        True                      False                     False                    \n",
      "fitting                   43                        True                      False                     False                    \n",
      "and                       44                        True                      False                     False                    \n",
      "proper                    45                        True                      False                     False                    \n",
      ".                         46                        False                     True                      False                    \n",
      "\n",
      "\n",
      "                        47                        False                     False                     False                    \n",
      "With                      48                        True                      False                     False                    \n",
      "malice                    49                        True                      False                     False                    \n",
      "toward                    50                        True                      False                     False                    \n",
      "none                      51                        True                      False                     False                    \n",
      "with                      52                        True                      False                     False                    \n",
      "charity                   53                        True                      False                     False                    \n",
      "for                       54                        True                      False                     False                    \n",
      "all                       55                        True                      False                     False                    \n",
      "with                      56                        True                      False                     False                    \n",
      "firmness                  57                        True                      False                     False                    \n",
      "in                        58                        True                      False                     False                    \n",
      "the                       59                        True                      False                     False                    \n",
      "right                     60                        True                      False                     False                    \n",
      "as                        61                        True                      False                     False                    \n",
      "God                       62                        True                      False                     False                    \n",
      "gives                     63                        True                      False                     False                    \n",
      "us                        64                        True                      False                     False                    \n",
      "to                        65                        True                      False                     False                    \n",
      "see                       66                        True                      False                     False                    \n",
      "the                       67                        True                      False                     False                    \n",
      "right                     68                        True                      False                     False                    \n",
      "let                       69                        True                      False                     False                    \n",
      "us                        70                        True                      False                     False                    \n",
      "strive                    71                        True                      False                     False                    \n",
      "on                        72                        True                      False                     False                    \n",
      "to                        73                        True                      False                     False                    \n",
      "finish                    74                        True                      False                     False                    \n",
      "the                       75                        True                      False                     False                    \n",
      "work                      76                        True                      False                     False                    \n",
      "we                        77                        True                      False                     False                    \n",
      "are                       78                        True                      False                     False                    \n",
      "in                        79                        True                      False                     False                    \n",
      "to                        80                        True                      False                     False                    \n",
      "bind                      81                        True                      False                     False                    \n",
      "up                        82                        True                      False                     False                    \n",
      "the                       83                        True                      False                     False                    \n",
      "nation                    84                        True                      False                     False                    \n",
      "'s                        85                        False                     False                     False                    \n",
      "wounds                    86                        True                      False                     False                    \n",
      ",                         87                        False                     True                      False                    \n",
      "to                        88                        True                      False                     False                    \n",
      "care                      89                        True                      False                     False                    \n",
      "for                       90                        True                      False                     False                    \n",
      "him                       91                        True                      False                     False                    \n",
      "who                       92                        True                      False                     False                    \n",
      "shall                     93                        True                      False                     False                    \n",
      "have                      94                        True                      False                     False                    \n",
      "borne                     95                        True                      False                     False                    \n",
      "the                       96                        True                      False                     False                    \n",
      "battle                    97                        True                      False                     False                    \n",
      "and                       98                        True                      False                     False                    \n",
      "for                       99                        True                      False                     False                    \n",
      "his                       100                       True                      False                     False                    \n",
      "widow                     101                       True                      False                     False                    \n",
      "and                       102                       True                      False                     False                    \n",
      "his                       103                       True                      False                     False                    \n",
      "orphan                    104                       True                      False                     False                    \n",
      "~                         105                       False                     False                     False                    \n",
      "to                        106                       True                      False                     False                    \n",
      "do                        107                       True                      False                     False                    \n",
      "all                       108                       True                      False                     False                    \n",
      "which                     109                       True                      False                     False                    \n",
      "may                       110                       True                      False                     False                    \n",
      "achieve                   111                       True                      False                     False                    \n",
      "and                       112                       True                      False                     False                    \n",
      "cherish                   113                       True                      False                     False                    \n",
      "a                         114                       True                      False                     False                    \n",
      "just                      115                       True                      False                     False                    \n",
      "and                       116                       True                      False                     False                    \n",
      "lasting                   117                       True                      False                     False                    \n",
      "peace                     118                       True                      False                     False                    \n",
      "among                     119                       True                      False                     False                    \n",
      "ourselves                 120                       True                      False                     False                    \n",
      "and                       121                       True                      False                     False                    \n",
      "with                      122                       True                      False                     False                    \n",
      "all                       123                       True                      False                     False                    \n",
      "nations                   124                       True                      False                     False                    \n",
      ".                         125                       False                     True                      False                    \n"
     ]
    }
   ],
   "source": [
    "## token attributes\n",
    "print(f\"{\"Token\":25}\",f\"{\"Is Alphanumeric?\":25}\",f\"{\"Is Punctuation?\":25}\",f\"{\"Like Number?\":25}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{str(token):25}\",f\"{str(token.i):25}\",f\"{str(token.is_alpha):25}\",f\"{str(token.is_punct):25}\",f\"{str(token.like_num):25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ac948365-6a1d-4d99-ac2d-31467dd58eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fellow, countrymen, second, appearing, oath, presidential, office, occasion, extended, address, statement, somewhat, detail, course, pursued, fitting, proper, \n",
      "\n",
      ", malice, charity, firmness, right, God, gives, right, let, strive, finish, work, bind, nation, wounds, care, shall, borne, battle, widow, orphan, ~, achieve, cherish, lasting, peace, nations]\n"
     ]
    }
   ],
   "source": [
    "##Stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print([token for token in doc if not (token.is_stop or token.is_punct)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4c42b8b1-723f-4741-868b-9453e0fb76de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow | fellow\n",
      "countrymen | countryman\n",
      "appearing | appear\n",
      "is | be\n",
      "was | be\n",
      "Then | then\n",
      "pursued | pursue\n",
      "seemed | seem\n",
      "With | with\n",
      "gives | give\n",
      "us | we\n",
      "us | we\n",
      "are | be\n",
      "wounds | wound\n",
      "him | he\n",
      "borne | bear\n",
      "lasting | last\n",
      "nations | nation\n"
     ]
    }
   ],
   "source": [
    "##Lemmatisation\n",
    "for token in doc:\n",
    "    if str(token)!=str(token.lemma_):\n",
    "        print(token,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "710849e6-a802-418e-ab5d-b3021fb058c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "countrymen | NNS | noun, plural | NOUN | noun\n",
      ": | : | punctuation mark, colon or ellipsis | PUNCT | punctuation\n",
      "at | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "this | DT | determiner | DET | determiner\n",
      "second | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "appearing | VBG | verb, gerund or present participle | VERB | verb\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "take | VB | verb, base form | VERB | verb\n",
      "the | DT | determiner | DET | determiner\n",
      "oath | NN | noun, singular or mass | NOUN | noun\n",
      "of | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "the | DT | determiner | DET | determiner\n",
      "presidential | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "office | NN | noun, singular or mass | NOUN | noun\n",
      "there | EX | existential there | PRON | pronoun\n",
      "is | VBZ | verb, 3rd person singular present | VERB | verb\n",
      "less | JJR | adjective, comparative | ADJ | adjective\n",
      "occasion | NN | noun, singular or mass | NOUN | noun\n",
      "for | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "an | DT | determiner | DET | determiner\n",
      "extended | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "address | NN | noun, singular or mass | NOUN | noun\n",
      "than | IN | conjunction, subordinating or preposition | SCONJ | subordinating conjunction\n",
      "there | EX | existential there | PRON | pronoun\n",
      "was | VBD | verb, past tense | VERB | verb\n",
      "at | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "the | DT | determiner | DET | determiner\n",
      "first | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      ". | . | punctuation mark, sentence closer | PUNCT | punctuation\n",
      "Then | RB | adverb | ADV | adverb\n",
      "a | DT | determiner | DET | determiner\n",
      "statement | NN | noun, singular or mass | NOUN | noun\n",
      "somewhat | RB | adverb | ADV | adverb\n",
      "in | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "detail | NN | noun, singular or mass | NOUN | noun\n",
      "of | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "a | DT | determiner | DET | determiner\n",
      "course | NN | noun, singular or mass | NOUN | noun\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "be | VB | verb, base form | AUX | auxiliary\n",
      "pursued | VBN | verb, past participle | VERB | verb\n",
      "seemed | VBD | verb, past tense | VERB | verb\n",
      "fitting | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "proper | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      ". | . | punctuation mark, sentence closer | PUNCT | punctuation\n",
      "\n",
      "\n",
      " | _SP | whitespace | SPACE | space\n",
      "With | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "malice | NN | noun, singular or mass | NOUN | noun\n",
      "toward | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "none | NN | noun, singular or mass | NOUN | noun\n",
      "with | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "charity | NN | noun, singular or mass | NOUN | noun\n",
      "for | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "all | DT | determiner | PRON | pronoun\n",
      "with | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "firmness | NN | noun, singular or mass | NOUN | noun\n",
      "in | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "the | DT | determiner | DET | determiner\n",
      "right | NN | noun, singular or mass | NOUN | noun\n",
      "as | IN | conjunction, subordinating or preposition | SCONJ | subordinating conjunction\n",
      "God | NNP | noun, proper singular | PROPN | proper noun\n",
      "gives | VBZ | verb, 3rd person singular present | VERB | verb\n",
      "us | PRP | pronoun, personal | PRON | pronoun\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "see | VB | verb, base form | VERB | verb\n",
      "the | DT | determiner | DET | determiner\n",
      "right | NN | noun, singular or mass | NOUN | noun\n",
      "let | VB | verb, base form | VERB | verb\n",
      "us | PRP | pronoun, personal | PRON | pronoun\n",
      "strive | VB | verb, base form | VERB | verb\n",
      "on | RP | adverb, particle | ADP | adposition\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "finish | VB | verb, base form | VERB | verb\n",
      "the | DT | determiner | DET | determiner\n",
      "work | NN | noun, singular or mass | NOUN | noun\n",
      "we | PRP | pronoun, personal | PRON | pronoun\n",
      "are | VBP | verb, non-3rd person singular present | AUX | auxiliary\n",
      "in | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "bind | VB | verb, base form | VERB | verb\n",
      "up | RP | adverb, particle | ADP | adposition\n",
      "the | DT | determiner | DET | determiner\n",
      "nation | NN | noun, singular or mass | NOUN | noun\n",
      "'s | POS | possessive ending | PART | particle\n",
      "wounds | NNS | noun, plural | NOUN | noun\n",
      ", | , | punctuation mark, comma | PUNCT | punctuation\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "care | VB | verb, base form | VERB | verb\n",
      "for | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "him | PRP | pronoun, personal | PRON | pronoun\n",
      "who | WP | wh-pronoun, personal | PRON | pronoun\n",
      "shall | MD | verb, modal auxiliary | AUX | auxiliary\n",
      "have | VB | verb, base form | AUX | auxiliary\n",
      "borne | VBN | verb, past participle | VERB | verb\n",
      "the | DT | determiner | DET | determiner\n",
      "battle | NN | noun, singular or mass | NOUN | noun\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "for | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "his | PRP$ | pronoun, possessive | PRON | pronoun\n",
      "widow | NN | noun, singular or mass | NOUN | noun\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "his | PRP$ | pronoun, possessive | PRON | pronoun\n",
      "orphan | NN | noun, singular or mass | NOUN | noun\n",
      "~ | , | punctuation mark, comma | PUNCT | punctuation\n",
      "to | TO | infinitival \"to\" | PART | particle\n",
      "do | VB | verb, base form | VERB | verb\n",
      "all | DT | determiner | PRON | pronoun\n",
      "which | WDT | wh-determiner | PRON | pronoun\n",
      "may | MD | verb, modal auxiliary | AUX | auxiliary\n",
      "achieve | VB | verb, base form | VERB | verb\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "cherish | VB | verb, base form | VERB | verb\n",
      "a | DT | determiner | DET | determiner\n",
      "just | JJ | adjective (English), other noun-modifier (Chinese) | ADJ | adjective\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "lasting | VBG | verb, gerund or present participle | VERB | verb\n",
      "peace | NN | noun, singular or mass | NOUN | noun\n",
      "among | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "ourselves | PRP | pronoun, personal | PRON | pronoun\n",
      "and | CC | conjunction, coordinating | CCONJ | coordinating conjunction\n",
      "with | IN | conjunction, subordinating or preposition | ADP | adposition\n",
      "all | DT | determiner | DET | determiner\n",
      "nations | NNS | noun, plural | NOUN | noun\n",
      ". | . | punctuation mark, sentence closer | PUNCT | punctuation\n"
     ]
    }
   ],
   "source": [
    "## predicting parts of speech (POS) tags from trained pipeline package\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.tag_,\"|\",spacy.explain(token.tag_),\"|\",token.pos_,\"|\",spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "39fc5b8b-af2c-4665-80cd-986ca39c9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow | ADJ | amod | adjectival modifier | countrymen\n",
      "countrymen | NOUN | dep | unclassified dependent | is\n",
      ": | PUNCT | punct | punctuation | is\n",
      "at | ADP | prep | prepositional modifier | is\n",
      "this | DET | det | determiner | second\n",
      "second | ADJ | pobj | object of preposition | at\n",
      "appearing | VERB | pcomp | complement of preposition | at\n",
      "to | PART | aux | auxiliary | take\n",
      "take | VERB | xcomp | open clausal complement | appearing\n",
      "the | DET | det | determiner | oath\n",
      "oath | NOUN | dobj | direct object | take\n",
      "of | ADP | prep | prepositional modifier | oath\n",
      "the | DET | det | determiner | office\n",
      "presidential | ADJ | amod | adjectival modifier | office\n",
      "office | NOUN | pobj | object of preposition | of\n",
      "there | PRON | expl | expletive | is\n",
      "is | VERB | ROOT | root | is\n",
      "less | ADJ | amod | adjectival modifier | occasion\n",
      "occasion | NOUN | attr | attribute | is\n",
      "for | ADP | prep | prepositional modifier | occasion\n",
      "an | DET | det | determiner | address\n",
      "extended | ADJ | amod | adjectival modifier | address\n",
      "address | NOUN | pobj | object of preposition | for\n",
      "than | SCONJ | mark | marker | was\n",
      "there | PRON | expl | expletive | was\n",
      "was | VERB | advcl | adverbial clause modifier | occasion\n",
      "at | ADP | prep | prepositional modifier | was\n",
      "the | DET | det | determiner | first\n",
      "first | ADJ | pobj | object of preposition | at\n",
      ". | PUNCT | punct | punctuation | is\n",
      "Then | ADV | advmod | adverbial modifier | seemed\n",
      "a | DET | det | determiner | statement\n",
      "statement | NOUN | nsubj | nominal subject | seemed\n",
      "somewhat | ADV | advmod | adverbial modifier | in\n",
      "in | ADP | prep | prepositional modifier | statement\n",
      "detail | NOUN | pobj | object of preposition | in\n",
      "of | ADP | prep | prepositional modifier | detail\n",
      "a | DET | det | determiner | course\n",
      "course | NOUN | pobj | object of preposition | of\n",
      "to | PART | aux | auxiliary | pursued\n",
      "be | AUX | auxpass | auxiliary (passive) | pursued\n",
      "pursued | VERB | relcl | relative clause modifier | course\n",
      "seemed | VERB | ROOT | root | seemed\n",
      "fitting | ADJ | oprd | object predicate | seemed\n",
      "and | CCONJ | cc | coordinating conjunction | fitting\n",
      "proper | ADJ | conj | conjunct | fitting\n",
      ". | PUNCT | punct | punctuation | seemed\n",
      "\n",
      "\n",
      " | SPACE | dep | unclassified dependent | .\n",
      "With | ADP | ROOT | root | With\n",
      "malice | NOUN | pobj | object of preposition | With\n",
      "toward | ADP | prep | prepositional modifier | malice\n",
      "none | NOUN | pobj | object of preposition | toward\n",
      "with | ADP | prep | prepositional modifier | none\n",
      "charity | NOUN | pobj | object of preposition | with\n",
      "for | ADP | prep | prepositional modifier | malice\n",
      "all | PRON | pobj | object of preposition | for\n",
      "with | ADP | prep | prepositional modifier | malice\n",
      "firmness | NOUN | pobj | object of preposition | with\n",
      "in | ADP | prep | prepositional modifier | malice\n",
      "the | DET | det | determiner | right\n",
      "right | NOUN | pobj | object of preposition | in\n",
      "as | SCONJ | mark | marker | gives\n",
      "God | PROPN | nsubj | nominal subject | gives\n",
      "gives | VERB | advcl | adverbial clause modifier | malice\n",
      "us | PRON | dobj | direct object | gives\n",
      "to | PART | aux | auxiliary | see\n",
      "see | VERB | advcl | adverbial clause modifier | gives\n",
      "the | DET | det | determiner | right\n",
      "right | NOUN | nsubj | nominal subject | let\n",
      "let | VERB | ccomp | clausal complement | see\n",
      "us | PRON | nsubj | nominal subject | strive\n",
      "strive | VERB | ccomp | clausal complement | let\n",
      "on | ADP | prt | particle | strive\n",
      "to | PART | aux | auxiliary | finish\n",
      "finish | VERB | advcl | adverbial clause modifier | strive\n",
      "the | DET | det | determiner | work\n",
      "work | NOUN | dobj | direct object | finish\n",
      "we | PRON | nsubj | nominal subject | are\n",
      "are | AUX | relcl | relative clause modifier | work\n",
      "in | ADP | prep | prepositional modifier | are\n",
      "to | PART | aux | auxiliary | bind\n",
      "bind | VERB | advcl | adverbial clause modifier | are\n",
      "up | ADP | prt | particle | bind\n",
      "the | DET | det | determiner | nation\n",
      "nation | NOUN | poss | possession modifier | wounds\n",
      "'s | PART | case | case marking | nation\n",
      "wounds | NOUN | dobj | direct object | bind\n",
      ", | PUNCT | punct | punctuation | bind\n",
      "to | PART | aux | auxiliary | care\n",
      "care | VERB | advcl | adverbial clause modifier | finish\n",
      "for | ADP | prep | prepositional modifier | care\n",
      "him | PRON | pobj | object of preposition | for\n",
      "who | PRON | nsubj | nominal subject | borne\n",
      "shall | AUX | aux | auxiliary | borne\n",
      "have | AUX | aux | auxiliary | borne\n",
      "borne | VERB | relcl | relative clause modifier | him\n",
      "the | DET | det | determiner | battle\n",
      "battle | NOUN | dobj | direct object | borne\n",
      "and | CCONJ | cc | coordinating conjunction | borne\n",
      "for | ADP | conj | conjunct | borne\n",
      "his | PRON | poss | possession modifier | widow\n",
      "widow | NOUN | pobj | object of preposition | for\n",
      "and | CCONJ | cc | coordinating conjunction | widow\n",
      "his | PRON | poss | possession modifier | orphan\n",
      "orphan | NOUN | conj | conjunct | widow\n",
      "~ | PUNCT | punct | punctuation | borne\n",
      "to | PART | aux | auxiliary | do\n",
      "do | VERB | xcomp | open clausal complement | borne\n",
      "all | PRON | dobj | direct object | do\n",
      "which | PRON | nsubj | nominal subject | achieve\n",
      "may | AUX | aux | auxiliary | achieve\n",
      "achieve | VERB | relcl | relative clause modifier | all\n",
      "and | CCONJ | cc | coordinating conjunction | achieve\n",
      "cherish | VERB | conj | conjunct | achieve\n",
      "a | DET | det | determiner | peace\n",
      "just | ADJ | amod | adjectival modifier | peace\n",
      "and | CCONJ | cc | coordinating conjunction | just\n",
      "lasting | VERB | conj | conjunct | just\n",
      "peace | NOUN | dobj | direct object | cherish\n",
      "among | ADP | prep | prepositional modifier | cherish\n",
      "ourselves | PRON | pobj | object of preposition | among\n",
      "and | CCONJ | cc | coordinating conjunction | among\n",
      "with | ADP | conj | conjunct | among\n",
      "all | DET | det | determiner | nations\n",
      "nations | NOUN | pobj | object of preposition | with\n",
      ". | PUNCT | punct | punctuation | With\n"
     ]
    }
   ],
   "source": [
    "## predicting SYNTECTIC DEPENDENCIES from trained pipeline package\n",
    "##The .dep_ attribute returns the predicted dependency label.\n",
    "##The .head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\"|\",token.pos_,\"|\",token.dep_,\"|\",spacy.explain(token.dep_),\"|\",token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b4caf0e9-60b1-4010-b698-4e9b9b9dc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy import displacy\n",
    "\n",
    "#doc1=nlp(\"Gus is learning piano\")\n",
    "\n",
    "#displacy.serve(doc1,style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9d852484-c733-4489-8d54-25756b0cc435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second | ORDINAL | \"first\", \"second\", etc.\n",
      "first | ORDINAL | \"first\", \"second\", etc.\n"
     ]
    }
   ],
   "source": [
    "## Predicting NAMED ENTITIES\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"|\",ent.label_,\"|\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d323701c-d362-4369-8077-0a7b6a8695f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Fellow countrymen: at this \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " appearing to take the oath of the presidential office there is less occasion for an extended address than there was at the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       ". Then a statement somewhat in detail of a course to be pursued seemed fitting and proper.<br><br>With malice toward none with charity for all with firmness in the right as God gives us to see the right let us strive on to finish the work we are in to bind up the nation's wounds, to care for him who shall have borne the battle and for his widow and his orphan ~ to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Displacy NAMES ENTITIES\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "02a1892d-747e-46c1-bc08-8ca7ea23ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God gives us\n"
     ]
    }
   ],
   "source": [
    "## RULE BASED MATCHING\n",
    "## Match on Doc objects, Match on tokens and token attributes\n",
    "## Use a model's predictions (Noun vs Verb)\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process some text\n",
    "#doc1 = nlp(book)\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "# Match patterns are lists of dictionaries\n",
    "# Match using lexical attributes (IS_DIGIT, IS_PUNCT,LOWER)\n",
    "# Match using token attributes (LEMMA,POS)\n",
    "# Match using operators (\"OP\") or quantifiers (? (0,1time), + (1 or more),* (0 or more), ! (negation 0 times))\n",
    "pattern = [{\"LOWER\": \"god\"}, {\"LEMMA\": \"give\"},{\"POS\":\"PRON\"}]\n",
    "matcher.add(\"T\", [pattern])\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "edf80681-32e3-47ab-af04-5db153c770f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "3197928453018144401\n",
      "coffee\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "## Shared Vocab\n",
    "## spaCy stores all shared data in a vocabulary, the Vocab. This includes words, but also the labels schemes for tags and entities.\n",
    "## To save memory, all strings are encoded to hash IDs. \n",
    "\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "\n",
    "#look up hash value\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "# The doc also exposes the vocab and strings\n",
    "doc_hash = doc.vocab.strings[\"coffee\"]                 \n",
    "\n",
    "#look up string value\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "string = nlp.vocab.strings[3197928453018144401]           # If a word is not in the vocabulary, there's no way to get its string.\n",
    "\n",
    "print(coffee_hash)\n",
    "print(doc_hash)\n",
    "print(coffee_string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0afb8d8-a2bf-4861-ba2d-25809338d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "## Lexeme\n",
    "## Lexemes are context-independent entries in the vocabulary.\n",
    "## Lexemes don't have context-dependant entries like part-of-speech tags, dependencies or entity labels.\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f9448f02-4c90-4706-bee1-309ab4aac0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc>>  Hello world!\n",
      "span>>  Hello world\n",
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "## Create Doc, Span, entities\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(\"doc>> \",doc)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span = Span(doc, 0, 2, label=\"GREETING\")\n",
    "print(\"span>> \",span)\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span]\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "766b0b6a-334e-4e23-8462-d44319f6ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "## Best Practice on Dat Structures\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city than Sydney\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags   >> use native token attributes to keep things consistent, convert the results to strings as late as possible\n",
    "\"\"\"\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\"\"\"\n",
    "\n",
    "# Instead of below\n",
    "\"\"\"\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)\n",
    "\"\"\"\n",
    "\n",
    "# try this\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        if token.i + 1 < len(doc):\n",
    "            # Check if the next token is a verb\n",
    "            if doc[token.i + 1].pos_ == \"VERB\":\n",
    "                print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a6c69159-9470-42cf-9073-1695cf56e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc similarity: 0.8698332283318978\n",
      "token similarity: 0.685019850730896\n",
      "0.1821369691957915\n",
      "0.47190033157126826\n"
     ]
    }
   ],
   "source": [
    "## Word vectors and semantic similarity\n",
    "## use spaCy to predict how similar documents, spans or tokens are to each other\n",
    "\n",
    "## The Doc, Token and Span objects have a .similarity method that takes another object\n",
    "## returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "## use medium or large spacy pipeline (not small)\n",
    "\n",
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(\"doc similarity:\",doc1.similarity(doc2))\n",
    "\n",
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(\"token similarity:\",token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d81499ec-95e3-4c59-8312-bb2050c2ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1821369691957915\n",
      "0.47190033157126826\n"
     ]
    }
   ],
   "source": [
    "## You can also use the similarity methods to compare different types of objects. For example, a document and a token.\n",
    "\n",
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "print(doc.similarity(token))\n",
    "\n",
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "36298efe-7def-4024-982d-4d70b8694d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20778  -2.4151    0.36605   2.0139   -0.23752  -3.1952   -0.2952\n",
      "  1.2272   -3.4129   -0.54969   0.32634  -1.0813    0.55626   1.5195\n",
      "  0.97797  -3.1816   -0.37207  -0.86093   2.1509   -4.0845    0.035405\n",
      "  3.5702   -0.79413  -1.7025   -1.6371   -3.198    -1.9387    0.91166\n",
      "  0.85409   1.8039   -1.103    -2.5274    1.6365   -0.82082   1.0278\n",
      " -1.705     1.5511   -0.95633  -1.4702   -1.865    -0.19324  -0.49123\n",
      "  2.2361    2.2119    3.6654    1.7943   -0.20601   1.5483   -1.3964\n",
      " -0.50819   2.1288   -2.332     1.3539   -2.1917    1.8923    0.28472\n",
      "  0.54285   1.2309    0.26027   1.9542    1.1739   -0.40348   3.2028\n",
      "  0.75381  -2.7179   -1.3587   -1.1965   -2.0923    2.2855   -0.3058\n",
      " -0.63174   0.70083   0.16899   1.2325    0.97006  -0.23356  -2.094\n",
      " -1.737     3.6075   -1.511    -0.9135    0.53878   0.49268   0.44751\n",
      "  0.6315    1.4963    4.1725    2.1961   -1.2409    0.4214    2.9678\n",
      "  1.841     3.0133   -4.4652    0.96521  -0.29787   4.3386   -1.2527\n",
      " -1.7734   -3.5637   -0.20035  -3.3013    0.99951  -0.92888  -0.94594\n",
      "  1.5124   -3.9385    2.7935   -3.1042    3.3382    0.54513  -0.37663\n",
      "  2.5151    0.51468  -0.88907   1.011     3.4705   -3.6037    1.3702\n",
      "  2.3468    1.6674    1.3904   -2.8112    2.237    -1.0344   -0.57164\n",
      "  1.0641   -1.6919    1.958    -0.78305   0.14741   0.51083   1.8278\n",
      " -0.69638   0.90548   0.62282  -1.8315   -2.8587    0.48424  -2.0527\n",
      " -0.53808  -2.3472    1.0354   -1.8257   -0.3892   -0.24943   0.8651\n",
      " -1.5195    1.2166   -2.698    -0.96698   2.2175   -0.16089  -0.49677\n",
      " -0.19646   1.3284    4.0824    1.3919    0.80669  -1.0316   -0.28056\n",
      " -1.8632    0.47716  -0.53628   1.3853   -2.1755   -0.2354    2.4933\n",
      " -0.87255   1.4493   -0.10778  -0.44159   1.3462    4.4211   -1.8385\n",
      "  0.3985    0.47637  -0.60074   3.3583   -0.15006  -0.40495   2.7225\n",
      " -1.6297    0.86797  -4.1445   -2.7793    1.1535   -0.011691  0.9792\n",
      " -1.0141    0.80134   0.43642   1.4337    2.8927    0.82871  -1.1827\n",
      " -1.3838    2.3903   -0.89323   1.1461   -1.7435    0.8654   -0.27075\n",
      " -0.78698   1.5631   -0.5923    0.098082 -0.26682   1.6282   -0.77495\n",
      "  3.2552    1.7964   -1.4314    1.2336    2.3102   -1.6328    2.8366\n",
      " -0.71384   0.43967   1.5627    3.079    -0.922    -0.43981  -0.7659\n",
      "  1.9362   -2.2479    1.041     0.63206   1.5855    3.4097   -2.9204\n",
      " -1.4751   -0.59534  -1.688    -4.1362    2.745    -2.8515    3.6509\n",
      " -0.66993  -2.8794    2.0733    1.1779   -2.0307    2.595    -0.12246\n",
      "  1.5844    1.1855    0.022385 -2.2916   -2.2684   -2.7537    0.34981\n",
      " -4.6243   -0.96521  -1.1435   -2.8894   -0.12619   2.9577   -1.7227\n",
      "  0.24757   1.2149    3.5349   -0.95802   0.080346 -1.6553   -0.6734\n",
      "  2.2918   -1.8229   -1.1336    1.8884    2.4789   -0.66061   2.0529\n",
      " -0.76687   0.32362  -2.2579    0.91278   0.36231   0.61562  -0.15396\n",
      " -0.42917  -0.89848   0.17298  -0.76978  -2.0222   -1.7127   -1.5632\n",
      "  0.56631  -1.354     2.6261    1.9156   -1.5651    1.8315   -1.4257\n",
      " -1.6861   -0.51953   1.7635   -0.50722   1.388    -1.1012  ]\n"
     ]
    }
   ],
   "source": [
    "## Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "## Algorithms like Word2Vec (that are used to convert raw text to vectors) can be added to Spacy's pipeline\n",
    "## By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.\n",
    "## Doc and Span vectors default to average of token vectors\n",
    "\n",
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1fc50-49b3-4e4d-b20b-7032848953b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity depends on the context and what application needs to do\n",
    "## Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "## high similarity score because both texts express sentiment about cats\n",
    "## in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
